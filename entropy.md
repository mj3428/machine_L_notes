# 信息熵衍生  
## 相对熵（relative entropy）和交叉熵（cross entropy）
**相对熵又叫KL散度（Kullback-Leibler divergence），用于衡量两个概率分布的差异**，而交叉熵则是相对熵去掉一个项之后的值  
<div align=center>![](entropy.png)
